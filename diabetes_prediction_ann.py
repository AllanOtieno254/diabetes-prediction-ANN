# -*- coding: utf-8 -*-
"""diabetes-prediction-ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfbsOgkgmAQzjJUBOY5BmosrkkfrYDAQ
"""

from google.colab import drive
drive.mount('/content/ANN_Diabetes_prediction')

# Importing the pandas library for data manipulation and analysis
import pandas as pd

# Importing the numpy library for numerical computations
import numpy as np

# Importing LabelEncoder for encoding categorical labels into numerical values
# Importing StandardScaler for standardizing/normalizing numerical features
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Importing train_test_split to split data into training and testing sets
from sklearn.model_selection import train_test_split

# Importing TensorFlow, an open-source machine learning library
import tensorflow as tf

# Importing Sequential, a linear stack of layers for building a neural network
from tensorflow.keras import Sequential

# Importing Dense, a fully connected layer used in neural networks
from tensorflow.keras.layers import Dense

# Importing Dropout, a regularization technique to prevent overfitting by randomly dropping neurons during training
from tensorflow.keras.layers import Dropout

df = pd.read_csv('/content/ANN_Diabetes_prediction/MyDrive/Diabetes prediction/diabetes.csv')
(df)

"""Explanation

•	Here, we are splitting the data into training and testing sets using the train_test_split function of the sklearn.model_selection module. This function splits the data at a specific ratio (in this case, 80% for training and 20% for testing) and ensures that the split is done randomly, using the value of random_state to control randomness.

•	Set X contains all the columns of the dataframe, except the 'Outcome' column, which is the target variable we want to predict. Set y contains only the 'Outcome' column. The sets xtrain and ytrain are used to train the model, while xtest and ytest are used to evaluate the performance of the model.

"""

# Showing 5 rows from the dataset
print(df.head())

print("\n")

#Splitting the data into training and testing sets
X = df.drop('Outcome', axis = 1)
y = df['Outcome']

xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.2, random_state = 0)

print("Features/independent variables")
print(X)

print("\n")

print("Target/dependent variable")
print(y)

"""Explanation

•	Here, we are using the StandardScaler from the sklearn.preprocessing module to standardize numerical data. Patterning is a common technique when preparing data for training machine learning models. It transforms the data so that the mean is 0 and the standard deviation is 1, ensuring that all features have the same scale. This is important because many machine learning algorithms are sensitive to the scale of the data.

•	First, we create an instance of StandardScaler called scaler. We then use the fit_transform method to compute the standardization statistics (mean and standard deviation) from the xtrain training set, and then apply the transform to the training and test sets using the transform method. This ensures that the same standardization is applied to both sets, using the statistics computed on the training set.

"""

#standardizing the data
scaler = StandardScaler()
xtrain = scaler.fit_transform(xtrain)
xtest = scaler.transform(xtest)

"""Explanation

In this piece of code, we are creating a neural network model using TensorFlow. The model is defined as a sequence of stacked layers. Here is an explanation of each part:

•	Dense(32, activation='relu', input_shape=(xtrain.shape[1],)): This line creates a dense layer with 32 units (neurons) and ReLU activation function. The layer receives as input a shape tensor (xtrain.shape[1],), which corresponds to the format of the input data of the training set. This layer is the first layer of the model, so we specify the input format.

•	Dropout(0.1): This line adds a dropout layer with a rate of 0.1. Dropout is a regularization technique that helps prevent overfitting by randomly deactivating a fraction of neurons during training.

•	Dense(32, activation='relu'): This line creates another dense layer with 32 units and ReLU activation function. This is the second layer of the model, no need to specify the input format as the output from the previous layer is used as input.

•	Dropout(0.5): This line adds a second dropout layer with a rate of 0.5.

•	Dense(1, activation='sigmoid'): This line creates the output layer of the model with a single neuron and sigmoid activation function.

This layer is responsible for producing the binary output of the model (0 or 1), indicating the target class.

"""

#Create the model
model = Sequential([
    Dense(32, activation = 'relu', input_shape = (xtrain.shape[1],)),
    Dropout(0.1),
    Dense(32, activation = 'relu'),
    Dropout(0.5),
    Dense(1, activation = 'sigmoid')
])

"""Explanation

After creating the model, we need to compile it before training it. On the first line, we are setting the model build options:

•	loss='binary_crossentropy': We use the binary cross entropy as the loss function. This loss function is suitable for binary classification problems, where we are trying to predict one of two classes.

•	optimizer='adam': The Adam optimizer will be used to adjust model weights during training. Adam is a popular optimization algorithm that relies on stochastic gradient descent methods.

•	metrics=['accuracy']: In addition to the loss function, we also want to track the accuracy metric during model training and evaluation. Accuracy is a common measure for evaluating classification model performance.

On the second line, we are printing a model summary, which displays the architecture of the neural network in tabular form. The summary includes information about the input and output format of each layer, the total number of trainable parameters, and the overall model summary.

"""

#Model Compliation
model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.summary()

"""Explanation

In this part of the code, we are training the neural network model. Here is an explanation of the different parts:

•	xtrain and ytrain are the training data, where xtrain contains the resources (inputs) and ytrain contains the corresponding labels (outputs). This data is used to adjust model weights during training.

•	epochs is the number of times the model will go through the entire training set. Each epoch consists of a cycle of going through the training data and adjusting the model weights.

•	batch_size is the number of training examples used in a single iteration. The training set is divided into smaller batches and adjustment of model weights is performed after each batch.

•	validation_data = (xtest, ytest) specifies the validation data to be used during training. This data is used to evaluate the model's performance on an independent dataset during training. xtest are the test resources and ytest are the corresponding labels.

"""

#Training the model
model.fit(xtrain, ytrain, epochs = 20, batch_size = 16, validation_data = (xtest, ytest))

"""Explanation

In this part of the code, we are evaluating the performance of the trained model using the test data. Here is an explanation of the different parts:

•	model.evaluate(xtest, ytest) calculates the loss and accuracy of the model in relation to the test data. Loss is a measure of how well the model is performing the task, while accuracy is the proportion of test examples correctly classified by the model.

•	loss is the loss calculated by the model on the test data.

•	accuracy is the accuracy calculated by the model on the test data.

•	print(f'Test loss: {loss}') prints the loss calculated during the evaluation of the test data.

•	print(f'Test accuracy: {accuracy}') prints the accuracy calculated during the evaluation of the test data.

This information is useful for understanding the performance of the trained model and evaluating its ability to generalize to previously unseen data.

"""

# Model Results
loss, accuracy = model.evaluate(xtest, ytest)
print(f'Test loss: {loss:.4f}')
print(f'Test accuracy: {accuracy:.4f}')

from tensorflow.keras.models import load_model
from sklearn.preprocessing import StandardScaler
import numpy as np

# Load the trained model
model_path = "/content/pima_diabetes_model.h5"  # Assuming model is in the same path
loaded_model = load_model(model_path)

# Load the dataset without outcome for prediction
test_data_path = "/content/ANN_Diabetes_prediction/MyDrive/Diabetes prediction/diabetes_testing_data.csv"
new_data = pd.read_csv(test_data_path)

# Simulating scaler (You should use the same scaler used during training)
scaler = StandardScaler()
scaler.fit(new_data)  # This should ideally be fitted on training data, but for demo, we're using test data

# Scale the new data
new_data_scaled = scaler.transform(new_data)

# Predict using the loaded model
predictions = loaded_model.predict(new_data_scaled)

# Convert the output from probability to class (0 or 1)
predicted_classes = (predictions > 0.5).astype(int)

# Create a DataFrame for results (Since we don't have actual labels, we'll assume it)
results_df = new_data.copy()
results_df["Predicted"] = predicted_classes

# Save the results DataFrame
# Define the correct file path
results_file_path = "/content/diabetes_predictions.csv"

# Save the results DataFrame to a CSV file
results_df.to_csv(results_file_path, index=False)

print(f"Predictions saved to: {results_file_path}")

(results_df)